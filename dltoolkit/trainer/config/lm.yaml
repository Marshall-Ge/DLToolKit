# specify the default per-component configs
defaults:

  # <folder_name>@<field_name>.<field_name>: <yaml_file_name>
  # strategy: trainer/config/strategy/accelerate.yaml
  - distributed@strategy : accelerate

  # optim: trainer/config/optim/adam.yaml
  - optim@optim : adam

  # If the new behavior works for your application, append _self_ to the end of the Defaults List.
  # If your application requires the previous behavior, insert _self_ as the first item in your Defaults List.
  - _self_

trainer:
  # Global training batch size
  train_batch_size: 32

  # Number of epochs to train
  max_epochs: 1

  # lr scheduler type
  lr_scheduler: "cosine_with_min_lr"

  # learning rate
  learning_rate: 0.001

  # learning rate warmup ratio
  lr_warmup_ratio: 0.03

  # evaluation steps
  eval_steps: -1

  # save steps
  save_steps: -1

  # logging steps
  log_steps: 1

model:
  # model type
  type: "causal_lm"

  # model name registered or huggingface model path
  name_or_path: "facebook/opt-125m"

data:
  # The field in the dataset where the text is located. Default is 'text'.
  text_key: text

  # dataset name registered or huggingface dataset path
  name_or_path: "Self-GRIT/wikitext-2-raw-v1-preprocessed"

  # dataset split to use
  split: train

  # fraction of data to use for training
  probs: "1.0"

  # max number of samples to use
  max_samples:

  # max length of text input
  max_len: 512

  # batch size for training dataloader
  drop_last: False

# random seed for initialization
seed: 42

# use model scope for initialization
use_ms: False

# load_checkpoint
load_checkpoint: True

# path to checkpoint to load
ckpt_path: './ckpt/lm/'

# directory to save models
save_path: './ckpt/'

# max number of checkpoints to save
max_ckpt_num: 3

# max checkpoint memory in MB
max_ckpt_mem: 1000


tracker:
    # use wandb for logging
    use_wandb:

    # wandb project name
    wandb_project: "dltoolkit"

    # wandb run name
    wandb_run_name: "lm"

    # wandb entity or organization name
    wandb_org: "your_wandb_org"

    # wandb group name
    wandb_group: "lm_experiments"

    # use tensorboard for logging
    use_tensorboard: './logs/'

# hydra:
#  # hydra output directory
#  output_subdir: output
#
#  # logger directory
#  run:
#    dir: .